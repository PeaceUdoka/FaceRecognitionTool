{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f007674-e2ed-41cf-ba94-0efa4c1b8563",
   "metadata": {},
   "source": [
    "## Collecting and Saving Images as LMDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc1a718-2586-431d-8ab9-eba7259d663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import lmdb\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c2ff8a9-384e-4316-9f9c-d49a163db22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\3023620433.py:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your first name:  KC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving faces: 100%|████████████████████████████████████████████████████████████████| 1000/1000 [01:05<00:00, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 1000 faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def capture_faces(num_faces=1000, face_size=(128, 128)):\n",
    "    \n",
    "    name = str(input(\"Please enter your first name: \"))\n",
    "    output_path = os.path.join(\"data\", f\"{name}.lmdb\")\n",
    "    # Initialize face detector\n",
    "    facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Create LMDB environment\n",
    "    map_size = num_faces * 1024 * 1024 * 3  \n",
    "    env = lmdb.open(output_path, map_size=map_size)\n",
    "\n",
    "    # Initialize webcam\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    \n",
    "    saved_count = 0\n",
    "    progress = tqdm(total=num_faces, desc=\"Saving faces\")\n",
    "\n",
    "    try:\n",
    "        with env.begin(write=True) as txn:\n",
    "            while saved_count < num_faces:\n",
    "                ret, frame = cam.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                # Detect faces\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Extract and resize face\n",
    "                    face_img = frame[y:y+h, x:x+w]\n",
    "                    resized_face = cv2.resize(face_img, face_size)\n",
    "\n",
    "                    # Store in LMDB\n",
    "                    key = f\"face_{saved_count:08d}\".encode()\n",
    "                    txn.put(key, pickle.dumps(resized_face))\n",
    "                    \n",
    "                    saved_count += 1\n",
    "                    progress.update(1)\n",
    "                    \n",
    "                    # Display current face count on the frame\n",
    "                    cv2.putText(frame, str(saved_count), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,255), 1)\n",
    "        \n",
    "                    # Draw rectangle around detected face\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 1)\n",
    "              \n",
    "                # Show preview\n",
    "                cv2.imshow('Face Capture', frame)\n",
    "                \n",
    "                # Exit on key ('q') pressed or completion (100 faces collected)\n",
    "                k=cv2.waitKey(1)\n",
    "                if k==ord('q') or len(faces_data)==1000:\n",
    "                    break\n",
    "\n",
    "    finally:\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        progress.close()\n",
    "        print(f\"\\nSaved {saved_count} faces\")\n",
    "\n",
    "\n",
    "\n",
    "# Capture 1000 faces to database\n",
    "capture_faces()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7366e177-88e4-4207-85e9-5e25cff8ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\2113577972.py:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  lmdb_path = \"data\\Peace.lmdb\"\n"
     ]
    }
   ],
   "source": [
    "def view_image(lmdb_path, key_to_view):\n",
    "\n",
    "    # Open LMDB environment in read-only mode\n",
    "    env = lmdb.open(lmdb_path, readonly=True)\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        # Retrieve the serialized image data using the key\n",
    "        value = txn.get(key_to_view.encode())\n",
    "        if value is None:\n",
    "            print(f\"No data found for key: {key_to_view}\")\n",
    "            return\n",
    "        \n",
    "        # Deserialize the data back into a numpy array\n",
    "        image = pickle.loads(value)\n",
    "        \n",
    "        # Display the image using OpenCV\n",
    "        cv2.imshow(f\"Image: {key_to_view}\", image)\n",
    "        cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "lmdb_path = \"data\\Peace.lmdb\" \n",
    "key_to_view = \"face_00000001\"  \n",
    "view_image(lmdb_path, key_to_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844095f-6a31-4366-8d07-ed334333191a",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c9e428-6126-4570-baa3-ab75472bf089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(lmdb_path, features, labels):\n",
    "   \n",
    "    env = lmdb.open(lmdb_path, readonly=True)\n",
    "    \n",
    "    pattern = r\"\\\\(.*)\\.\"\n",
    "    match = re.search(pattern, lmdb_path)\n",
    "    name = match.group(1)[1:]\n",
    "    \n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "        for key, value in cursor:\n",
    "            key_str = key.decode()  # Decode key from bytes to string\n",
    "            key_str += name\n",
    "            image = pickle.loads(value)  # Deserialize image data back into a numpy array\n",
    "            labels.append(name)\n",
    "            features.append(image)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f37651-789b-4e30-91c9-65d88f8c3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"data\"\n",
    "features = []\n",
    "labels = []\n",
    "for path in os.listdir(dir):\n",
    "    lmdb_path = os.path.join(dir,path)\n",
    "    features, labels = get_images(lmdb_path, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181b769f-6275-4801-8ec2-99f142f39fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891a25c0-c980-47ef-a494-ea762c35b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c87f73-c5cd-457f-afa1-d465c64ef7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features) \n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029b8ea4-b1b9-4431-9de1-406d006f1677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0bd57-fec0-4c29-9254-19e7de0d72f7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5485e8ab-be56-4e39-aea2-34977eae04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D, \n",
    "    Activation, \n",
    "    BatchNormalization, \n",
    "    MaxPooling2D, \n",
    "    Dropout, \n",
    "    Flatten, \n",
    "    Dense\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,TensorBoard,ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "938b0935-2ef8-4590-bedc-ba480eed9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70dcdf99-93a5-4a71-8c1e-e2072749f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating model on GPU\n",
    "def build():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (128, 128, 3)))\n",
    "    # Feature Learning Layers with Kernel size (3x3), Step size (1 pixel)\n",
    "    model.add(Conv2D(32,(3, 3),strides=(1, 1),padding='same'))\n",
    "    model.add(Activation('relu'))# Activation function\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Flattening tensors\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully-Connected Layers\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation = 'sigmoid')) # binaryClassification layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dea87252-4488-40e8-9606-57b1e94461f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f9e93af-9427-4e00-9aa5-53828920eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy', # change to sparse_categorical_crossentropy for multiclass where each sample can only belong to one class\n",
    "              metrics = ['accuracy']) # Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64056746-1309-4d5f-a282-38d0d04f331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an Early Stopping and Model Checkpoints\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', \n",
    "                               min_delta = 0.01,\n",
    "                               mode = 'min',\n",
    "                               patience = 7,\n",
    "                               start_from_epoch = 20,\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                            monitor = 'val_loss',\n",
    "                            save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c01d196-b848-415c-bd25-b98e06ced66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.8439 - loss: 2.8288 - val_accuracy: 0.5100 - val_loss: 3468.0435\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.9929 - loss: 0.1702 - val_accuracy: 0.5100 - val_loss: 3473.4360\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.9990 - loss: 0.0125 - val_accuracy: 0.5100 - val_loss: 932.8412\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.9992 - loss: 0.0396 - val_accuracy: 0.5167 - val_loss: 277.9084\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.7100 - val_loss: 55.5258\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.9993 - loss: 0.0061 - val_accuracy: 0.8633 - val_loss: 10.6264\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 16s/step - accuracy: 1.0000 - loss: 2.9549e-08 - val_accuracy: 0.9600 - val_loss: 3.4463\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 0.9991 - loss: 0.0168 - val_accuracy: 0.9967 - val_loss: 1.9283\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.9985 - loss: 0.0224 - val_accuracy: 0.9967 - val_loss: 0.9018\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 2.1334e-16 - val_accuracy: 0.9967 - val_loss: 0.4938\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.8309e-14 - val_accuracy: 0.9967 - val_loss: 0.4308\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.9444e-16 - val_accuracy: 0.9967 - val_loss: 0.4134\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.2038e-19 - val_accuracy: 0.9967 - val_loss: 0.4065\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.1377e-11 - val_accuracy: 0.9967 - val_loss: 0.4052\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 6.2902e-20 - val_accuracy: 0.9967 - val_loss: 0.4056\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.3519e-13 - val_accuracy: 0.9967 - val_loss: 0.4066\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.5957e-18 - val_accuracy: 0.9967 - val_loss: 0.4083\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.7869e-12 - val_accuracy: 0.9967 - val_loss: 0.4096\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.3814e-08 - val_accuracy: 0.9967 - val_loss: 0.4120\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.1929e-19 - val_accuracy: 0.9967 - val_loss: 0.4136\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 5.0793e-11 - val_accuracy: 0.9967 - val_loss: 0.4167\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.7770e-12 - val_accuracy: 0.9967 - val_loss: 0.4182\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.4351e-11 - val_accuracy: 0.9967 - val_loss: 0.4192\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.9474e-13 - val_accuracy: 0.9967 - val_loss: 0.4204\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.0988e-13 - val_accuracy: 0.9967 - val_loss: 0.4209\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.5060e-15 - val_accuracy: 0.9967 - val_loss: 0.4213\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.7407e-17 - val_accuracy: 0.9967 - val_loss: 0.4221\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 0.9996 - loss: 0.0073 - val_accuracy: 0.8900 - val_loss: 11.0563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f15f3ac110>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=  1, validation_data = (X_val, y_val), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec7595a1-3ddf-4703-90b6-c49ddf5ea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9a8da-932f-495d-b3c4-4c10545796ee",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d059312-674f-4491-a49b-2fabe9f84833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 537ms/step - accuracy: 1.0000 - loss: 2.6742e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.005305198091392e-12, 1.0]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test, batch_size=64, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae9b85-395a-4180-8391-80d2778bfe7b",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef286b9-a28a-4790-8827-8550e5448fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13eda99c-ee5c-4b92-bddb-ceebd8bc3c76",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1b5544b-7a41-40c3-ba21-cf4fc9fc1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('face_recog_model.pkl', 'wb') as file:  # 'wb' for writing in binary mode\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84c4d2-348f-4420-bfd9-7fbe3ae69a31",
   "metadata": {},
   "source": [
    "## Test Case\n",
    "\n",
    "Get files of pictures and labels for test predictions and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c5245b7-42ee-4e6c-920c-888f1c85f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 350ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c449c5d9-5c3b-4f82-9a66-29c3db9b4171",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classification_report(y_test, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2626\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2491\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2492\u001b[0m     {\n\u001b[0;32m   2493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2517\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2518\u001b[0m ):\n\u001b[0;32m   2519\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2520\u001b[0m \n\u001b[0;32m   2521\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2623\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2626\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   2628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2629\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m             type_true, type_pred\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "059d8cbd-03bc-4b9f-b748-4ba0ddd04790",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# plot confusion matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cm1 \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test,y_pred)\n\u001b[0;32m      3\u001b[0m ConfusionMatrixDisplay(cm1)\u001b[38;5;241m.\u001b[39mplot()\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    248\u001b[0m     {\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m             type_true, type_pred\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm1 = confusion_matrix(y_test,y_pred)\n",
    "ConfusionMatrixDisplay(cm1).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff293b10-5309-4f10-9ec3-7d317ba24a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468224af-02fb-4041-a853-61b2733cac12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538b5be-e811-4e99-b7f5-e824d9158382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61a621-4ea6-4ece-9491-eff27f0e7be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c79c24-8101-4eae-a906-34cc61bd7e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73242b97-da3d-432f-88ee-c0ba89609874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6577bca-b72d-47e8-9d33-d00097a33c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f08c196d-0ae2-4a9d-9e21-c53734bb39f2",
   "metadata": {},
   "source": [
    "## Saving Images as BLOB to Postgresql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a308bcaf-4d1b-41b8-a78c-2babf3144058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\3216854562.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect=cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your first name:  Peace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\3216854562.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect=cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m ret,frame\u001b[38;5;241m=\u001b[39mvideo\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert frame to grayscale (face detection works better on grayscale)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m gray\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Detect faces using the haar cascade pre-trained classifier\u001b[39;00m\n\u001b[0;32m     22\u001b[0m faces\u001b[38;5;241m=\u001b[39mfacedetect\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m1.3\u001b[39m ,\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "name = str(input(\"Please enter your first name: \"))\n",
    "\n",
    "video=cv2.VideoCapture(0)\n",
    "\n",
    "facedetect=cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "faces_data = []  # Empty list to store resized face images\n",
    "i = 0  # Counter to track processed frames\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video source (webcam/file)\n",
    "    ret,frame=video.read()\n",
    "    \n",
    "    # Convert frame to grayscale (face detection works better on grayscale)\n",
    "    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using the haar cascade pre-trained classifier\n",
    "    faces=facedetect.detectMultiScale(gray, 1.3 ,5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Crop the face region from the frame\n",
    "        crop_img = frame[y:y+h, x:x+w, :]\n",
    "        \n",
    "        # Resize cropped face to 50x50 pixels \n",
    "        resized_img = cv2.resize(crop_img, (50, 50))\n",
    "        \n",
    "        # Collect every 10th face until 100 faces are stored\n",
    "        if len(faces_data) <= 10 and i % 10 == 0:\n",
    "            faces_data.append(resized_img)\n",
    "            \n",
    "        # Increment frame counter\n",
    "        i += 1\n",
    "        \n",
    "        # Display current face count on the frame\n",
    "        cv2.putText(frame, str(len(faces_data)), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,255), 1)\n",
    "        \n",
    "        # Draw rectangle around detected face\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 1)\n",
    "\n",
    "    # Show the processed frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Check for exit key ('q') or completion (100 faces collected)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==ord('q') or len(faces_data)==10:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#faces_data=np.asarray(faces_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecfc5f-50b6-4417-b017-69306b8abaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "\n",
    "# Connect to the postgreSQL database \n",
    "def create_connection(): \n",
    "    conn = psycopg2.connect(dbname='FaceRecog', \n",
    "                            user='postgres', \n",
    "                            password='Ifechukwu', #st.secret[]\n",
    "                            host='localhost', \n",
    "                            port='5432') \n",
    "    # Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn, curr \n",
    "  \n",
    "def create_table(): \n",
    "    \n",
    "    try: \n",
    "        # Get the cursor object from the connection object \n",
    "        conn, curr = create_connection() \n",
    "        try: \n",
    "            # Fire the CREATE query \n",
    "            curr.execute(\"DROP TABLE authorized\") \n",
    "            curr.execute(\"CREATE TABLE IF NOT EXISTS authorized(personID TEXT, name TEXT, faceImg BYTEA)\") \n",
    "              \n",
    "        except(Exception, psycopg2.Error) as error: \n",
    "            # Print exception \n",
    "            print(\"Error while creating authorized table\", error) \n",
    "        finally: \n",
    "            # Close the connection object \n",
    "            conn.commit() \n",
    "            conn.close() \n",
    "    finally: \n",
    "        # Since we do not have to do anything here we will pass \n",
    "        pass\n",
    "  \n",
    "def write_blob(personID,face,name): \n",
    "    try:  \n",
    "        # Read database configuration \n",
    "        conn, cursor = create_connection() \n",
    "        try:            \n",
    "            # Execute the INSERT statement \n",
    "            # Convert the image data to Binary \n",
    "            cursor.execute(\"INSERT INTO authorized (personID,name,faceImg) \" + \"VALUES(%s,%s,%s)\", (personID,name, psycopg2.Binary(face))) \n",
    "            # Commit the changes to the database \n",
    "            conn.commit() \n",
    "        except (Exception, psycopg2.DatabaseError) as error: \n",
    "            print(\"Error while inserting data in authorized table\", error) \n",
    "        finally: \n",
    "            # Close the connection object \n",
    "            conn.close() \n",
    "    finally: \n",
    "        # Since we do not have to do anything here we will pass \n",
    "        pass\n",
    "    print(\"Saved faces\")\n",
    "        \n",
    "# Call the create table method       \n",
    "create_table() \n",
    "# Prepare sample data, of images, from local drive \n",
    "for i, face in enumerate(faces_data):\n",
    "    personID = name+str(i)\n",
    "    write_blob(personID,face,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f13eb7-20c0-411c-8632-2ce88d83c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "# Connect to the postgreSQL database \n",
    "def create_connection(): \n",
    "    conn = psycopg2.connect(dbname='FaceRecog', \n",
    "                            user='postgres', \n",
    "                            password='Ifechukwu', #st.secret[]\n",
    "                            host='localhost', \n",
    "                            port='5432') \n",
    "    # Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn, curr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e778e83-491d-4035-bb88-29a08250d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PersonID   name                                               face\n",
      "0    Peace0  Peace  [b'r', b'\\x82', b'\\xb3', b'p', b'\\x7f', b'\\xb0...\n",
      "1    Peace1  Peace  [b'~', b'~', b'~', b'\\x80', b'\\x80', b'\\x80', ...\n",
      "2    Peace2  Peace  [b'\\x8a', b'\\x8a', b'\\x8a', b'\\x8a', b'\\x8c', ...\n",
      "3    Peace3  Peace  [b'\\x83', b'\\x82', b'~', b'\\x84', b'\\x83', b'\\...\n",
      "4    Peace4  Peace  [b'\\x7f', b'}', b'|', b'\\x80', b'\\x81', b'\\x7f...\n",
      "5    Peace5  Peace  [b'\\x80', b'\\x80', b'\\x80', b'\\x7f', b'{', b'\\...\n",
      "6    Peace6  Peace  [b'\\xd8', b'\\xe0', b'\\xe6', b'\\xc9', b'\\xca', ...\n",
      "7    Peace7  Peace  [b'K', b'J', b'N', b'L', b'K', b'O', b'L', b'K...\n",
      "8    Peace8  Peace  [b'\\x95', b'\\x97', b'\\x92', b'\\x8f', b'\\x93', ...\n",
      "9    Peace0  Peace  [b'\\xc1', b'\\xc5', b'\\xd0', b'\\xc1', b'\\xc5', ...\n",
      "10   Peace1  Peace  [b'v', b'{', b'{', b'y', b'}', b'~', b'w', b'{...\n",
      "11   Peace2  Peace  [b'z', b'\\x7f', b'~', b'~', b'\\x84', b'\\x80', ...\n",
      "12   Peace3  Peace  [b'y', b'~', b'y', b'\\x81', b'\\x82', b'\\x80', ...\n",
      "13   Peace4  Peace  [b'{', b'\\x80', b'\\x7f', b'x', b'}', b'|', b'|...\n",
      "14   Peace5  Peace  [b'y', b'}', b'|', b'{', b'\\x81', b'|', b'w', ...\n",
      "15   Peace6  Peace  [b'\\x80', b'\\x84', b'\\x7f', b'\\x80', b'\\x83', ...\n",
      "16   Peace7  Peace  [b'\\x81', b'\\x85', b'\\x7f', b'\\x81', b'\\x84', ...\n",
      "17   Peace8  Peace  [b'}', b'\\x85', b'{', b'~', b'\\x86', b'|', b'\\...\n",
      "18   Peace9  Peace  [b'~', b'\\x82', b'w', b'\\x7f', b'\\x83', b'x', ...\n",
      "19   Peace0  Peace  [b'\\xe8', b'\\xed', b'\\xde', b'\\xe4', b'\\xe8', ...\n",
      "20   Peace1  Peace  [b'\\xae', b'\\xa5', b'\\x95', b'\\x7f', b'\\x80', ...\n",
      "21   Peace2  Peace  [b'z', b'}', b'\\x81', b'}', b'\\x80', b'\\x84', ...\n",
      "22   Peace3  Peace  [b'z', b'}', b'\\x85', b'\\x8e', b'\\x94', b'\\x9a...\n",
      "23   Peace4  Peace  [b'\\x9c', b'\\x9f', b'\\xa6', b'}', b'\\x81', b'\\...\n",
      "24   Peace5  Peace  [b'\\x88', b'\\x85', b'\\x95', b'\\x8a', b'\\x8a', ...\n",
      "25   Peace6  Peace  [b'\\x82', b'y', b'\\x82', b'\\x90', b'\\x8d', b'\\...\n",
      "26   Peace7  Peace  [b'\\x9f', b'\\x9d', b'\\x9d', b'\\x9c', b'\\x9a', ...\n",
      "27   Peace8  Peace  [b'\\x96', b'\\x92', b'\\x9c', b'\\x91', b'\\x91', ...\n",
      "28   Peace9  Peace  [b'\\xa0', b'\\x9b', b'\\x9f', b'\\x98', b'\\x94', ...\n",
      "29   Peace0  Peace  [b'\\xb9', b'\\xb9', b'\\xb9', b'\\xb5', b'\\xb7', ...\n",
      "30   Peace1  Peace  [b'\\xa4', b'\\xa1', b'\\x9d', b'\\xa4', b'\\xa1', ...\n",
      "31   Peace2  Peace  [b'\\x96', b'\\x93', b'\\x8f', b'\\x95', b'\\x91', ...\n",
      "32   Peace3  Peace  [b'\\xb5', b'\\xb9', b'\\xb8', b'\\x97', b'\\x99', ...\n",
      "33   Peace4  Peace  [b'\\xac', b'\\xb5', b'\\xb8', b'\\xab', b'\\xb3', ...\n",
      "34   Peace5  Peace  [b'\\xae', b'\\xb4', b'\\xb6', b'\\xad', b'\\xb5', ...\n",
      "35   Peace6  Peace  [b'\\xab', b'\\xb3', b'\\xb0', b'\\xaa', b'\\xaf', ...\n",
      "36   Peace7  Peace  [b'\\x92', b'\\x94', b'\\x95', b'\\x8f', b'\\x95', ...\n",
      "37   Peace8  Peace  [b'\\xb4', b'\\xbd', b'\\xb4', b'\\xb2', b'\\xbd', ...\n",
      "38   Peace9  Peace  [b'\\xb6', b'\\xbf', b'\\xbc', b'\\xb9', b'\\xc4', ...\n",
      "39  Peace10  Peace  [b'\\xaf', b'\\xaf', b'\\xa9', b'\\xab', b'\\xab', ...\n",
      "40  Peace11  Peace  [b'O', b'^', b'w', b'\\x7f', b'\\x8a', b'\\x96', ...\n",
      "41  Peace12  Peace  [b'Z', b'k', b'~', b'|', b'\\x86', b'\\x93', b'\\...\n",
      "42  Peace13  Peace  [b'\\x8c', b'\\x88', b'\\x87', b'\\x91', b'\\x8e', ...\n",
      "43  Peace14  Peace  [b'\\x99', b'\\x9e', b'\\x96', b'\\xa1', b'\\xa7', ...\n",
      "44  Peace15  Peace  [b'w', b'\\x84', b'\\x8c', b'e', b't', b'z', b'j...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def read_img(): \n",
    "    conn, curr = create_connection() \n",
    "    curr.execute(\"SELECT * FROM authorized\")\n",
    "    rows = curr.fetchall()\n",
    "    df = pd.DataFrame(rows, columns=[\"PersonID\",\"name\",\"face\"])\n",
    "    return df\n",
    "    \n",
    "df = read_img()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f7b173-3687-4a2b-b021-2ba6872eeb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import base64\n",
    "head = df.head()\n",
    "def Display_images(head):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images in head.face:\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            blob = base64.b64encode(images[i]).decode('utf-8')\n",
    "            #plt.title(class_names[labels[i]])\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "Display_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd36305-52d8-473d-a1ca-86dbe8a73555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33120f-55bb-43cd-913c-21cde786277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dde976-81b6-4ba2-8a55-96e628b38ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b6df2-f2d2-4012-b0f0-5503ad31697a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c21b9-37c1-429d-9e31-061719d1583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://youtu.be/NwvTh-gkdfs?si=i88ZDcKSbz8mlzDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf99965-0f58-4dba-b936-73948801efa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
