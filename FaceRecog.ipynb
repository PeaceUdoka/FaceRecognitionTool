{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f007674-e2ed-41cf-ba94-0efa4c1b8563",
   "metadata": {},
   "source": [
    "## Collecting and Saving Images as LMDB files\n",
    "\n",
    "Note: KC = 0, Peace = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc1a718-2586-431d-8ab9-eba7259d663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import lmdb\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c2ff8a9-384e-4316-9f9c-d49a163db22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\3023620433.py:12: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your first name:  KC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving faces: 100%|████████████████████████████████████████████████████████████████| 1000/1000 [01:05<00:00, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 1000 faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def capture_faces(num_faces=1000, face_size=(128, 128)):\n",
    "    \n",
    "    name = str(input(\"Please enter your first name: \"))\n",
    "    output_path = os.path.join(\"data\", f\"{name}.lmdb\")\n",
    "    # Initialize face detector\n",
    "    facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Create LMDB environment\n",
    "    map_size = num_faces * 1024 * 1024 * 3  \n",
    "    env = lmdb.open(output_path, map_size=map_size)\n",
    "\n",
    "    # Initialize webcam\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    \n",
    "    saved_count = 0\n",
    "    progress = tqdm(total=num_faces, desc=\"Saving faces\")\n",
    "\n",
    "    try:\n",
    "        with env.begin(write=True) as txn:\n",
    "            while saved_count < num_faces:\n",
    "                ret, frame = cam.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                # Detect faces\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Extract and resize face\n",
    "                    face_img = frame[y:y+h, x:x+w]\n",
    "                    resized_face = cv2.resize(face_img, face_size)\n",
    "\n",
    "                    # Store in LMDB\n",
    "                    key = f\"face_{saved_count:08d}\".encode()\n",
    "                    txn.put(key, pickle.dumps(resized_face))\n",
    "                    \n",
    "                    saved_count += 1\n",
    "                    progress.update(1)\n",
    "                    \n",
    "                    # Display current face count on the frame\n",
    "                    cv2.putText(frame, str(saved_count), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,255), 1)\n",
    "        \n",
    "                    # Draw rectangle around detected face\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 1)\n",
    "              \n",
    "                # Show preview\n",
    "                cv2.imshow('Face Capture', frame)\n",
    "                \n",
    "                # Exit on key ('q') pressed or completion (100 faces collected)\n",
    "                k=cv2.waitKey(1)\n",
    "                if k==ord('q') or len(faces_data)==1000:\n",
    "                    break\n",
    "\n",
    "    finally:\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        progress.close()\n",
    "        print(f\"\\nSaved {saved_count} faces\")\n",
    "\n",
    "\n",
    "\n",
    "# Capture 1000 faces to database\n",
    "capture_faces()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7366e177-88e4-4207-85e9-5e25cff8ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15484\\2113577972.py:22: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  lmdb_path = \"data\\Peace.lmdb\"\n"
     ]
    }
   ],
   "source": [
    "def view_image(lmdb_path, key_to_view):\n",
    "\n",
    "    # Open LMDB environment in read-only mode\n",
    "    env = lmdb.open(lmdb_path, readonly=True)\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        # Retrieve the serialized image data using the key\n",
    "        value = txn.get(key_to_view.encode())\n",
    "        if value is None:\n",
    "            print(f\"No data found for key: {key_to_view}\")\n",
    "            return\n",
    "        \n",
    "        # Deserialize the data back into a numpy array\n",
    "        image = pickle.loads(value)\n",
    "        \n",
    "        # Display the image using OpenCV\n",
    "        cv2.imshow(f\"Image: {key_to_view}\", image)\n",
    "        cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "lmdb_path = \"data\\Peace.lmdb\" \n",
    "key_to_view = \"face_00000001\"  \n",
    "view_image(lmdb_path, key_to_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844095f-6a31-4366-8d07-ed334333191a",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c9e428-6126-4570-baa3-ab75472bf089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(lmdb_path, features, labels):\n",
    "   \n",
    "    env = lmdb.open(lmdb_path, readonly=True)\n",
    "    \n",
    "    pattern = r\"\\\\(.*)\\.\"\n",
    "    match = re.search(pattern, lmdb_path)\n",
    "    name = match.group(1)[:]\n",
    "    \n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "        for key, value in cursor:\n",
    "            key_str = key.decode()  # Decode key from bytes to string\n",
    "            key_str += name\n",
    "            image = pickle.loads(value)  # Deserialize image data back into a numpy array\n",
    "            labels.append(name)\n",
    "            features.append(image)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f37651-789b-4e30-91c9-65d88f8c3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"data\"\n",
    "features = []\n",
    "labels = []\n",
    "for path in os.listdir(dir):\n",
    "    lmdb_path = os.path.join(dir,path)\n",
    "    features, labels = get_images(lmdb_path, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181b769f-6275-4801-8ec2-99f142f39fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "891a25c0-c980-47ef-a494-ea762c35b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c87f73-c5cd-457f-afa1-d465c64ef7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features) \n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "029b8ea4-b1b9-4431-9de1-406d006f1677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0bd57-fec0-4c29-9254-19e7de0d72f7",
   "metadata": {},
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5485e8ab-be56-4e39-aea2-34977eae04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D, \n",
    "    Activation, \n",
    "    BatchNormalization, \n",
    "    MaxPooling2D, \n",
    "    Dropout, \n",
    "    Flatten, \n",
    "    Dense\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,TensorBoard,ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938b0935-2ef8-4590-bedc-ba480eed9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70dcdf99-93a5-4a71-8c1e-e2072749f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating model on CPU\n",
    "def build():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (128, 128, 3)))\n",
    "    # Feature Learning Layers with Kernel size (3x3), Step size (1 pixel)\n",
    "    model.add(Conv2D(32,(3, 3),strides=(1, 1),padding='same'))\n",
    "    model.add(Activation('relu'))# Activation function\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (3,3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Flattening tensors\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully-Connected Layers\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation = 'sigmoid')) # binaryClassification layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dea87252-4488-40e8-9606-57b1e94461f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f9e93af-9427-4e00-9aa5-53828920eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy', # change to sparse_categorical_crossentropy for multiclass where each sample can only belong to one class\n",
    "              metrics = ['accuracy']) # Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64056746-1309-4d5f-a282-38d0d04f331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an Early Stopping and Model Checkpoints\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', \n",
    "                               min_delta = 0.01,\n",
    "                               mode = 'min',\n",
    "                               patience = 7,\n",
    "                               start_from_epoch = 20,\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                            monitor = 'val_loss',\n",
    "                            save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c01d196-b848-415c-bd25-b98e06ced66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.8439 - loss: 2.8288 - val_accuracy: 0.5100 - val_loss: 3468.0435\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.9929 - loss: 0.1702 - val_accuracy: 0.5100 - val_loss: 3473.4360\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.9990 - loss: 0.0125 - val_accuracy: 0.5100 - val_loss: 932.8412\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.9992 - loss: 0.0396 - val_accuracy: 0.5167 - val_loss: 277.9084\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.7100 - val_loss: 55.5258\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.9993 - loss: 0.0061 - val_accuracy: 0.8633 - val_loss: 10.6264\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 16s/step - accuracy: 1.0000 - loss: 2.9549e-08 - val_accuracy: 0.9600 - val_loss: 3.4463\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 0.9991 - loss: 0.0168 - val_accuracy: 0.9967 - val_loss: 1.9283\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.9985 - loss: 0.0224 - val_accuracy: 0.9967 - val_loss: 0.9018\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 2.1334e-16 - val_accuracy: 0.9967 - val_loss: 0.4938\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.8309e-14 - val_accuracy: 0.9967 - val_loss: 0.4308\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.9444e-16 - val_accuracy: 0.9967 - val_loss: 0.4134\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.2038e-19 - val_accuracy: 0.9967 - val_loss: 0.4065\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.1377e-11 - val_accuracy: 0.9967 - val_loss: 0.4052\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 6.2902e-20 - val_accuracy: 0.9967 - val_loss: 0.4056\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.3519e-13 - val_accuracy: 0.9967 - val_loss: 0.4066\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.5957e-18 - val_accuracy: 0.9967 - val_loss: 0.4083\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.7869e-12 - val_accuracy: 0.9967 - val_loss: 0.4096\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.3814e-08 - val_accuracy: 0.9967 - val_loss: 0.4120\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.1929e-19 - val_accuracy: 0.9967 - val_loss: 0.4136\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 5.0793e-11 - val_accuracy: 0.9967 - val_loss: 0.4167\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.7770e-12 - val_accuracy: 0.9967 - val_loss: 0.4182\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.4351e-11 - val_accuracy: 0.9967 - val_loss: 0.4192\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.9474e-13 - val_accuracy: 0.9967 - val_loss: 0.4204\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.0988e-13 - val_accuracy: 0.9967 - val_loss: 0.4209\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.5060e-15 - val_accuracy: 0.9967 - val_loss: 0.4213\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.7407e-17 - val_accuracy: 0.9967 - val_loss: 0.4221\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 0.9996 - loss: 0.0073 - val_accuracy: 0.8900 - val_loss: 11.0563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f15f3ac110>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=  1, validation_data = (X_val, y_val), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec7595a1-3ddf-4703-90b6-c49ddf5ea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9a8da-932f-495d-b3c4-4c10545796ee",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d059312-674f-4491-a49b-2fabe9f84833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372ms/step - accuracy: 0.9989 - loss: 0.0056  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016857018694281578, 0.996666669845581]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test, batch_size=64, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda99c-ee5c-4b92-bddb-ceebd8bc3c76",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1b5544b-7a41-40c3-ba21-cf4fc9fc1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('face_recog_model.pkl', 'wb') as file:  # 'wb' for writing in binary mode\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff293b10-5309-4f10-9ec3-7d317ba24a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a84c4d2-348f-4420-bfd9-7fbe3ae69a31",
   "metadata": {},
   "source": [
    "## Test Case\n",
    "\n",
    "Get files of pictures and labels for test predictions and evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5edd275-4862-4fda-869b-19bb68d6c3d8",
   "metadata": {},
   "source": [
    "### Collect Face from Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eed4c5aa-02d2-4f8d-aa62-661d256923db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_12520\\2663604764.py:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    }
   ],
   "source": [
    "def scan_face(num_faces=1, face_size=(128, 128)):\n",
    "    \n",
    "    face_data = []\n",
    "    # Initialize face detector\n",
    "    facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "      # Read a frame from the video source (webcam/file)\n",
    "      ret,frame =  cam.read()\n",
    "      if not ret:\n",
    "        print(\"Error reading frame from camera\")\n",
    "        continue\n",
    "        \n",
    "      # Convert frame to grayscale (face detection works better on grayscale)\n",
    "      gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "      # Detect faces\n",
    "      faces = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "      \n",
    "      for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around detected face\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 1) \n",
    "        # Extract and resize face\n",
    "        cropped_face = frame[y:y+h, x:x+w]\n",
    "        resized_face = cv2.resize(cropped_face , face_size)\n",
    "        face_data.append(np.array(resized_face))\n",
    "        # Display feedback\n",
    "        cv2.putText(frame, f\"Capturing \", \n",
    "                   (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        # Show preview\n",
    "      cv2.imshow('Face Capture', frame)\n",
    "      if len(face_data) != 1:\n",
    "          cv2.waitKey(0)\n",
    "          print(\"No face found\")\n",
    "          break\n",
    "      else: \n",
    "          cv2.waitKey(0)\n",
    "          print(f\"\\nFace Scanned\")\n",
    "          break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    return face_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47c47919-e772-4154-9979-f2e2112247e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Face Scanned\n"
     ]
    }
   ],
   "source": [
    "face = scan_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7031d-d4c0-4953-ac39-45e9665da978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "face_dim = np.expand_dims(face[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c3484f4-3217-41c7-b5fa-3edc38cb76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 128, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "110aa523-7ad9-4c20-8a87-b94c52fa44ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(face_dim).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ad29c-892c-45db-90dc-02b450f14949",
   "metadata": {},
   "source": [
    "Correctly predicted the identity of the face. 1 is for Peace (that's me!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29255678-c2c0-4141-a0d6-41c0cd10cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(face_dim):\n",
    "    prediction = best_model.predict(face_dim).astype(int)\n",
    "    if prediction == 0:\n",
    "        print(\"Access granted. Welcome KC!\")\n",
    "    elif prediction == 1:\n",
    "        print(\"Access granted. Welcome Peace!\")\n",
    "    else:\n",
    "        print(\"Access denied!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a111fdf2-16e9-4961-b3b1-f3d9278312b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Access granted. Welcome Peace!\n"
     ]
    }
   ],
   "source": [
    "predict(face_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78b445-6482-42b9-bccf-c70693d86e45",
   "metadata": {},
   "source": [
    "### Collect Image from Directory\n",
    "\n",
    "Let's try an image from my gallery of KC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a538b5be-e811-4e99-b7f5-e824d9158382",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = \"pic1.jpg\"\n",
    "img = cv2.imread(img_input)\n",
    "cv2.imshow(\"image\", img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855a1be-3542-4fda-82f3-9312c52c27c7",
   "metadata": {},
   "source": [
    "## Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11c79c24-8101-4eae-a906-34cc61bd7e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5212\\1381088494.py:5: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n"
     ]
    }
   ],
   "source": [
    "def preprocess(img, face_size=(128, 128)):\n",
    "    img_array = []\n",
    "    \n",
    "    # Initialize face detector\n",
    "    facedetect = cv2.CascadeClassifier('models\\haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detect faces\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in face:\n",
    "        # Extract and resize face\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        resized_face = cv2.resize(face_img, face_size)\n",
    "        img_array.append(np.array(resized_face))\n",
    "    return img_array\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73242b97-da3d-432f-88ee-c0ba89609874",
   "metadata": {},
   "outputs": [],
   "source": [
    "face1 = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6577bca-b72d-47e8-9d33-d00097a33c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "face1_dim = np.expand_dims(face1[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e8b80b5-575e-4610-b1a4-6f838a54fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521ms/step\n",
      "Access granted. Welcome KC!\n"
     ]
    }
   ],
   "source": [
    "predict(face1_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe93a0-efc8-4a4f-99c6-0bd215103c5f",
   "metadata": {},
   "source": [
    "It works perfectly! Hurray!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
